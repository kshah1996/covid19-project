---
title: ""
author: "Emily Damone, Taylor Krajewski, Alex Quinter, Kushal Shah, Euphy Wu, Jonathan Zhang"
date: "4/22/2020"
output: html_document
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(prophet)
library(lme4)
```

<p align="center">
<img width="250" src="mask.png">
</p>


# Introduction

COVID-19 (Novel Coronavirus) is an infectious disease that was first identified in December 2019 in Wuhan, the capital city of Hubei province in the People's Republic of China. Since its discovery, it has currently spread to over 90% of countries in the world and has infected nearly 2,000,000 people.

Despite movement restrictions put in place by most countries, the disease continues to spread, and new cases are documented daily. However, the lack of symptoms for many infected with coronavirus along with lack of widespread testing has made it nearly impossible to determine how many new cases of Coronavirus there actually are each day. This makes modeling the pandemic extremely difficult. 

Many experts have made excellent models and predictions that inspired us to create our own. We have seen some excellent country-specific, or even US state-specific models that attempt to predict the number of new or total cases. In the model we will describe below, we attempted to predict the number of new cases for a country based on that country's previous number of documented cases, global health security index, percentage of population over 65 years old, and percentage of the population living in an urban setting. 


# Project Goal

To predict the number of new cases of Covid-19 a country can expect on a chosen day given that country's previous number of recorded cases and a few measurable variables specific to that country that may affect the spread of the virus.


# Data

## *Raw Data*

### Hopkins COVID-19 Data

Our initial dataset comes from the Johns Hopkins CSSE, a team which has been refreshing the dataset daily during the ongoing COVID-19 crisis. The dataset includes coronavirus total cases, recoveries, active cases, and deaths per day by country, with information for certain regions given in more detail (e.g. US states). 

For this project, the focus was confirmed cases. Dates from the original dataset wre standardized by relabeling the date at which a country passes 50 total cases as "Day 0", with every day past that used for modeling purposes. A variable for new cases was created by lagging over total cases. 

### World Bank Data
According to the CDC, "older adults and people who have severe underlying medical conditions like heart or lung disease or diabetes seem to be at higher risk for developing more serious complications from COVID-19 illness." 
As of mid-March, in the United States, more than 30% of COVID-19 cases were patients 65 years of age or older. 

Due to the apparent prevalence of the disease in the 65+ population, we decided to include the percentage of a country's population over the age of 65 in our model. 

We found these percentages from a World Bank dataset, last updated in 2019, that provides the percent of the population 65 years of age or older in each country.

### Global Health Security Index Data

The Global Health Security (GHS) Index is a comprehensive assessment of a country's health security capabilities. Countries receive an ovreall score as well as individual scores for each of the 6 categories:

<p align="center">
<img width="600" src="ghs image.png">
</p>


* *Prevention*: Prevention of the emergence or release of pathogens

* *Detection and Reporting*: Early detection and reporting for epidemics of potential international concern

* *Rapid Response*: Rapid response to and mitigation of the spread of an epidemic

* *Health System*: Sufficient and robust health system to treat the sick and protect health workers

* *Compliance with International Norms*: Commitments to improving national capacity, financing plans to address gaps, and adhering to global norms

* *Risk Environment*: Overall risk environment and country vulnerability to biological threats

(*GHSIndex.org*)

As the GHS Index was created in response to the 2014 West Africa Ebola epidemic in order to help global leaders better understand and measure a country's ability to prevent, detect, and respond to infectious disease threats, we found it an appropriate measure to include in modeling the current Coronavirus pandemic.

For this project, we have included a country's overall GHS Index as a covariate in our model. 


## *Data Processing*
(Data processing summary - should discuss joining Covid data with country data from the other sources mentioned above)

## *Finalized Data*

Below is a preview of what our final data set looks like. Each country has a row for each day of recorded cases from baseline (day of 50 cases) until April 3, 2020.

```{r include=FALSE}
library(data.table)
library(knitr)

dat2 <- readRDS("dat2.rds")
datDisplay = readRDS("dat3.rds")

us <- subset(datDisplay, Country.Region == "US", select = c(day, total_cases, new_cases, GHS_Score, AgeGEQ65, UrbanPop, TotalPop))

  
colnames(us) <- c("Day", "Total Cases", "New Cases", "GHS Index", "% 65+", "% Urban Setting", "Total Population (2018)")



```

```{r, echo=FALSE}

kable(us, rnames = FALSE,
          caption="United States Data")

```


# Packaging
(description of our R package - functions and datasets it holds)


# Methods

During the initial growth phase of a pandemic, the number of cases shows exponential growth. As time progresses, the number of new cases each day starts to decrease, and eventually we reach a final number of cases. Thus, the cumulative number of cases form an approximately linear relationship with time in the log-linear scale in the initial phases, but then plateaus after a certain timepoint.
It is common in Epidemiology to model this initial exponential growth and subsequent slow-down using a logistic growth model. The cumulative incidences $C(t)$ (the total number of cases by time $t$) can be approximated by

$$
C(t)=\frac{KC_0}{C_0+(K-C_0)e^{-rt}}
$$

where $r$ is the exponential growth rate, $K=\displaystyle \lim_{t \to \infty} C(t)$ and $C_0=C(0)$.

By taking the derivative of $C(t)$, we can estimate the change in number of total cases with respect to time. Hence, when looking between two subsequent time points, we can find the number of new cases.

The derivative of $C(t)$, the density of the logistic function, is $\frac{d}{dt}=\frac{rKC_0e^{rt}(K-C_0)}{((K-C_0)+C_0e^{rt})^2}$, which looks a lot like a normal distribution! Hence, we can use a normal distribution to approximate the logistic function. 


```{r, echo=FALSE}
# Graphing Logistic Growth, Derivative, and Gaussian Curve

#Logistic Growth (CDF)
L = function(x){
  plogis(x)
}

#Logistic Density (PDF)
DL = function(x){
    2*dlogis(x)
}

#Normal Density
N=function(x){
  2*dnorm(x,0,1.6)
}


x <- seq(-10, 10, by = 0.1)

colors <- c("purple", "orange", "blue")

#labels <- c("Total Cases (Logistic Growth Curve)","Derivative of Total Cases (Logistic Curve)", "Gaussian Curve")
labels <- c("Total Cases","New Cases", "Gaussian Curve")

#Plot Logistic Growth Curve
plot( L(x), yaxt='n', type="l", lty=1, xlab="Time (in days)",
  ylab=" ", main="Comparison of Functions", col="purple", lwd=3)
# Plot Logistic Density
lines( DL(x), yaxt='n', col="orange", lwd=3)
# Plot Gaussian Density
lines( N(x), yaxt='n', col="blue", lwd=3, lty=2)


legend("topleft", inset=0, title="Distributions",
  labels, lwd=2, lty=c(1, 1, 2), col=colors)


```

This led us to attempt to model number of new cases using a Gaussian curve.
We found this fit the number of new cases quite well for many countries, with some limitations of course. Since this density fit the number of new cases quite well in initial modeling, we decided to create a model that incorporates an $exp(time^2)$ term, and hence we included covariates for $time$ as well as $time^2$.


## *Poisson GLMM*


Let $y_{ij}$ be the number of new cases country $i$ has on day $j$. 

We assume that each $y_{ij} \sim Poisson(\lambda_{ij})$, where the mean and variance of $y_{ij}$ is assumed to be some $\lambda_{ij}$

Let the covariate vector pertaining to the $i^{th}$ countrh in the $j^{th}$ month be defined as $x_{ij} = (1, t_{ij}, t^2_{ij}, GHS, AgeGEQ65, UrbanPop)$.

We assume an intercept, $\beta_0$, in our model, and thus we have $\beta = (\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6)^T$.

We assumed counts can be modeled with a Poisson GLMM such that $log(\lambda_{ij})=x_{ij}\beta+\gamma_{i1}+\gamma_{i2}t_{ij} = x_{ij}\beta+z_{ij}\gamma_i$, where $b_i$ is a vector of unobserved country-level random effects. We assume that

$$\left(\begin{array}{cc}\gamma_{i1} \\\gamma_{i2}\end{array}\right) \sim N_2\bigg(\left(\begin{array}{cc}0 \\0\end{array}\right),\left(\begin{array}{cc}g_{11} & g_{12} \\g_{21} & g_{22}\end{array}\right)\bigg)$$

So, in our model, $\gamma_{i1}$ is the deviation of the $i^{th}$ country from the baseline number of new cases, which we have set to 50, and $\gamma_{i2}$ is the deviation of the $i^{th}$ country from the average effect of the covariates in our model on number of new cases.


As in all GLMMs, the random effects are unobservable. Therefore, in order to obtain the likelihood, we must integrate them out.

Thus, we must perform separate integrals for each $\left(\begin{array}{cc}\gamma_{i1} \\\gamma_{i2}\end{array}\right)$, which share the common distribution dtermined by $G=\left(\begin{array}{cc}g_{11} & g_{12} \\g_{21} & g_{22}\end{array}\right)$.

Thus, we write the likelihood:

$$L(\beta, G | y, \gamma)=\prod^{n}_{i=1}\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i | 0, G) = \prod^{n}_{i=1}\int\bigg[\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i|0,G)\bigg]d\gamma_i$$

and the log-likelihood:

$$l(\beta, G | y, \gamma)=\sum^{n}_{i=1}log\bigg[\int\bigg[\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i|0,G)\bigg]d\gamma_i\bigg]$$

where $f(y_{ij}|\lambda_{ij})$ is the Poisson PMF with mean $\lambda_{ij}$, as defined above, and $\phi(\gamma_i|0,G)$ is the bivariate Normal PDF with mean 0 and covariance matrix $G=\left(\begin{array}{cc}g_{11} & g_{12} \\g_{21} & g_{22}\end{array}\right)$.


We maximized this log-likelihood to obtain estimates for $\beta$ and $G$. We go about this approach using an MCEM algorithmn. We first define the complete data log-likelihood if the random effects were known.

\begin{align}
l_c(\beta, G | y, \gamma)&=\sum^{n}_{i=1}log\bigg[\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i|0,G)\bigg]\\

&=\sum^{n}_{i=1}\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ij}))+log(\phi(\gamma_i|0,G))\bigg]
\end{align}

In the E-step we need to evaluate the expectation of the complete data log-likelihood, the Q function.

\begin{align}
Q(\theta|\theta^{(t)})&=E[l_c(\theta)|\theta^{(t)}]\\

&=E\bigg[\sum^{n}_{i=1}\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ij}^{(t)}))+log(\phi(\gamma_i|0,G^{(t)}))\bigg]\bigg]\\

&=\sum^{n}_{i=1}\bigg[\int\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ij}^{(t)}))+log(\phi(\gamma_i|0,G^{(t)}))\bigg]f(\gamma_i|\beta^{(t)},G^{(t)})d\gamma_i\bigg]
\end{align}

where $\lambda_{ij}^{(t)}=e^{x_{ij}\beta^{(t)}+z_{ij}\gamma_{i}}$. If we could sample from the posterior distribution of $\gamma_i$ we could approximate this intergral through a MC approach.

\begin{align}
Q(\theta|\theta^{(t)})&=\frac{1}{M}\sum^{n}_{i=1}\bigg[\sum^{M}_{k=1}\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ijk}^{(t)}))+log(\phi(\Gamma_{ik}|0,G^{(t)}))\bigg]\bigg]\\
\end{align}

where $\Gamma_{ik}$ is one of the $M$ samples from the $i$-th country, and $\lambda_{ijk}^{(t)}=e^{x_{ij}\beta^{(t)}+z_{ij}\Gamma_{ik}}$. To sample from this multivariate posterior, we tried 3 different approaches: an independence random walk, a Metropolis-within-Gibbs random walk, and an adaptive Metropolis-within-Gibbs random walk. Of these 3 methods, we decided that the Metropolis-within-Gibbs random walk approach was the most suitable. We consider a new $X^*=x^{(t)}+\epsilon$ at each Gibbs step, where $\epsilon\sim N(0,0.01)$. Then we using the Metropolis Hastings ratio to determine if it is accepted or rejected into the Markov Chain.

Now for the M-step, once we have the Q-function, we need to maximize it with respect to $\beta$ and $G$. We see from the Q-function above, that after we draw the $M$ samples for each $i$, $\Gamma_{ik}$, the maximization process follows the maximization of parameters in a Poisson GLM with a few tweaks. First, each sample must be weighted $\frac{1}{M}$. Second, we need to add an offset that are the random effects $z_i\gamma_i$. Then using the glm function in R, we can easily perform the M-step. Afterwards, we start the E-step again and iterate until convergence.

### Model Prediction

To evaluate our model, we compare the predictions that our models made to the data from future days which were not used in the generation of our model. To facilitate this we need to be able to generate a prediction of the random effects given a country, or more specifically be able to estimate $\gamma_i|y_i,\beta,G$. We can use the following expectation as a good estimate.

\begin{align}
\hat{\gamma_i}&=E[\gamma_i|y_i,\hat{\beta},\hat{G}]\\
&=\int\gamma_if(\gamma_i|y_i,\hat{\beta},\hat{G})d\gamma_i
\end{align}
As in the E-step we can use a MC approach to estimate this mean as follows.
\begin{align}
E[\gamma_i|y_i,\hat{\beta},\hat{G}]&=\frac{1}{M}\sum^{M}_{k=1}\Gamma_{ik}
\end{align}

where $\Gamma_{ik}$ is one of the $M$ samples from the $i$-th country. Since we have already generated $\Gamma_{ik}$ from the E-step iterations above, we can take the mean of those Markov chains to estimate country specific random effects. The predicted number of new cases for country $i$ at day $j$ is

\begin{align}
E[y_{ij}|\gamma_i]&=\frac{1}{1+e^{-(x_{ij}\hat{\beta}+z_{ij}\hat{\gamma_i})}}
\end{align}

## *Machine Learning Methods*

Two modeling techniques were considered for the machine learning section: Random Forest and Prophet (time-series modeling). 

There is some existing literature for machine learning methods for longitudinal data, however, the extent is limited and recent method developments lack direct applications in R. For example, Ngufor et al. (2018) develop a "Mixed Effect Random Forest", but its implementation in R is still limited and produces vague error messages. Therefore, a regular random forest technique was applied to this dataset instead, as it is still a nonparametric technique and may be able to uncover the temporal structure of the dataset.

Additionally, we decided to consider a time-series modeling method called Prophet to further answer our scientific question of predicting new cases in future days. Prophet was originally developed by Facebook Research and is available in packages for both R and Python. The method was built for building time-series predictions for business forecasting problems, but we believe it may be used to predict future cases of Covid-19. Note, this method can only be used to model cases for one country at a time.

Prophet is based in a decomposable time series model as follows: 
$$y(t) = g(t) + s(t) + h(t) +\epsilon_t$$
$y(t)$ ~ total cases, $g(t)$ ~ non-periodic changes, $s(t)$~periodic changes (such as weekly or yearly), $h(t)$~holiday effects, $\epsilon_t$~$N(0,\sigma^2)$

While the method allows for seasonal and holiday effects, we do not include such in our model as we do not believe it applies to Covid-19 cases. 

We utilized a logistic growth model, which has the following form: 
$$g(t) = \frac{C(t)}{1 + \exp(-k(t-m))}$$ Where $C(t)$ is a function for the carrying capacity, $k(t)$ is the growth rate, and $m$ is an offset parameter. In typical time series models, C and k are constants, but an advantage of Prophet is allowing these to vary with time.

Prophet utilizes *L-BFGS* to determine parameter estimates. 

### Random Forest

Since country-specific random effects could not be incorporated, a random forest model was applied to the dataset as a whole, using `day`, `GHS_Score`, `AgeGEQ65`, and `UrbanPop` to predict new cases, with the hope that the country-specific variables may result in more country-specific predictions.

#### Model Evaluation

However, after running the random forest model, we observed that fits to the data and predictions were very inaccurate, especially for countries whose curves differed considerably from other countries.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(randomForest)

dat2_rf <- dat2 %>% select(Country.Region, day, GHS_Score, AgeGEQ65, UrbanPop, new_cases) %>%
  drop_na()
dat2_rf_X <- dat2_rf %>% select(day, GHS_Score, AgeGEQ65, UrbanPop)
dat2_rf_Y <- dat2_rf %>% select(new_cases)

rf1 <- randomForest(x = dat2_rf_X, y = dat2_rf$new_cases)

dat2_rf_preds <- dat2_rf %>% mutate(pred = predict(rf1))

### USA
dat2_rf_preds_usa <- dat2_rf_preds %>% filter(Country.Region == "US")

day <- 0:70
GHS_Score <- 83.5
AgeGEQ65 <- 15.80765
UrbanPop <- 82.256
dat2_rf_usa <- data.frame(day, GHS_Score, AgeGEQ65, UrbanPop)
dat2_rf_usa$pred <- predict(rf1, dat2_rf_usa)
dat2_rf_usa$new_cases <- c(dat2_rf_preds_usa$new_cases, rep(NA, 31))

ggplot(dat2_rf_usa, aes(x = day)) +
  geom_point(aes(y = new_cases)) +
  geom_line(aes(y = pred)) +
  labs(x = "Day", y = "New Cases") + 
  ggtitle("USA Predictions with Random Forest")

#sum(dat2_rf_preds_usa$pred)
#sum(dat2_rf_preds_usa$new_cases)

#imp <- importance(rf1)
#lattice::dotplot(tail(sort(imp[,1]),10), xlab="Mean Decrease Gini")

### Austria

dat2_rf_austria <- dat2_rf %>% filter(Country.Region == "Austria")
dat2_rf_austria_futurepreds <- dat2_rf_austria
dat2_rf_austria_futurepreds$day <- 29:57
dat2_rf_austria_futurepreds$pred <- predict(rf1, dat2_rf_austria_futurepreds)
dat2_rf_austria_futurepreds$new_cases <- NA

dat2_rf_austria_currentpreds <- dat2_rf_austria
dat2_rf_austria_currentpreds$pred <- predict(rf1, dat2_rf_austria_currentpreds)

austria <- rbind(dat2_rf_austria_currentpreds, dat2_rf_austria_futurepreds)
ggplot(austria, aes(x = day)) +
  geom_point(aes(y = new_cases)) +
  geom_line(aes(y = pred)) +
  labs(x = "Day", y = "New Cases") + 
  ggtitle("Austria Predictions with Random Forest")
```

For example, for the United States, as plotted above, the random forest model highly overpredicts early on and underpredicts as the situation worsens. Whereas the GLMM fit for the US was quite accurate (even with the US being quite an outlier in terms of number of daily new cases) because it was prespecified to be a bell curve, random forest struggles because its estimates for the United States are biased towards the outcome of all countries with a similar `GHS_Score`, `AgeGEQ65`, and `UrbanPop`. 

#### Limitations of Random Forest

In this setting, a random forest modeling approach (and the majority of available machine learning approaches) is limited by its inability to handle correlated, longitudinal data. The key limitations of random forest for this dataset are as follows:

* Countries with similar GHS scores and population structures still had vastly different disease curves and trajectories with respect to new COVID cases per day. It is nearly impossible to gather a collection of covariates that accounts for the differences in curve structure between every country. Whether such a set of covariates even exists is unlikely, let alone actually finding it. Therefore, without being able to predict for an individual country based off of its previous time points (as was the case with the longitudinal GLMM modeling approach), making country-specific predictions through a random forest model was unlikely to be fruitful.

* Although random forest's nonparametric modeling structure may seem promising here, we noticed that generally, the number of new cases for the majority of countries follows a bell-shaped curve with time, although with vastly different parameters that specify the curve. This situation is indeed well-suited for parametric statistical methods that limit each country's model to a specified distribution, while still allowing the parameters of that distribution to vary wildly. Extra assumptions that were needed for GLMM, in this case, were helpful, because they ensured, for example, that the predicted new cases for any single day is nonnegative. This rigidity resulted in extrapolations (future time points) at least being reasonable. 

In one sense, the GLMM approach allows for flexibility where it is needed (in variation amongst the country-specific parameters), while also remaining rigid in a way that leads to reasonable predictions (giving every country the same bell-curve shape, which is a valid assumption based on disease literature and is reflected in the COVID data so far). To make future predictions for time points outside the current dataset, fixing a bellcurve-type structure helps.

# Results

## *Poisson GLMM*

Running the model we have on the dataset lead to parameter estimates as follows (code not explicity ran here, it was ran on the cluster)

```
Iter: 65 Qf: -86681.995 g11: 1.896946 g12: -0.067057 g22: 0.010074 beta0: 0.846 beta1:0.202 beta2:-0.005 beta3:0.028 beta4 :0.010 beta5:-0.001 eps:0.000104
Iter: 66 Qf: -86700.912 g11: 1.928345 g12: -0.068573 g22: 0.010125 beta0: 0.846 beta1:0.202 beta2:-0.005 beta3:0.028 beta4 :0.010 beta5:-0.001 eps:0.000218
Iter: 67 Qf: -86700.985 g11: 1.922342 g12: -0.067895 g22: 0.010088 beta0: 0.844 beta1:0.202 beta2:-0.005 beta3:0.028 beta4 :0.010 beta5:-0.001 eps:0.000001
```

with final parameter estimates of $\hat{\beta}=(0.844,0.202,-0.005,0.028,0.010,-0.001)$ and $\hat{G}=\left(\begin{array}{cc}1.922 & -0.068 \\-0.068 & 0.010\end{array}\right)$.

We can compare this to estimates from the glmer package in R.

```{r echo=FALSE}
## more data processing
dat = readRDS("dat2.rds")
# remove na data
dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
# modify china new_cases day 0 since it was NA previously
dat[402,5]=548
dat$ID <- dat %>% group_indices(Country.Region)

for (i in 1:max(dat$ID)) {
  if (sum(dat$ID==i) < 5) {
    dat<- dat[!(dat$ID==i),]
  }
}

dat$ID <- dat %>% group_indices(Country.Region)
```
```{r}
glmm1 <- glmer(new_cases ~ day + day2 + GHS_Score + AgeGEQ65 + UrbanPop + (day | Country.Region), data = dat, family = poisson)
summary(glmm1)
```

We see that the estimates are very close to what glmer reports. The difference could be attributed to the Laplace approximation that the glmer package uses.

Below is the predictions of our GLMM model for the United States. The model was built on the first 40 days, and then we use the predictions to project the next 8 days. Since the original data is from the past, we also know the true observed number of new cases in the United States as well. The dashed lines indicated time points that were not included in the generation of our model.

```{r echo = FALSE, warning=FALSE, message=FALSE}
## more data processing
dat = readRDS("dat2.rds")
# remove na data
dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
# modify china new_cases day 0 since it was NA previously
dat[402,5]=548
dat$ID <- dat %>% group_indices(Country.Region)

for (i in 1:max(dat$ID)) {
  if (sum(dat$ID==i) < 5) {
    dat<- dat[!(dat$ID==i),]
  }
}

dat$ID <- dat %>% group_indices(Country.Region)

# unique country list
order = unique(dat$Country.Region)

gamma <- read.table("longleaf/glmm_mwg_rw_gamma_1.txt", header = F, skip = 1)
gamma2 <- as.matrix(gamma[,2:3])
M <- 1000

fix_mwg <- c(0.844,0.202,-0.005,0.028,0.010,-0.001)
##US
country = 97
country_name = 'US'

ran_mwg <- c(mean(gamma2[((country-1)*M+1):(country*M),1]),mean(gamma2[((country-1)*M+1):(country*M),2]),0,0,0,0)
coef_mwg <- fix_mwg + ran_mwg

tday = 48
GHS_Score <- ((dat %>% filter(Country.Region==country_name))[1,])$GHS_Score
AgeGEQ65 <- ((dat %>% filter(Country.Region==country_name))[1,])$AgeGEQ65
UrbanPop <- ((dat %>% filter(Country.Region==country_name))[1,])$UrbanPop
pred <- tibble(int=rep(1,tday)) %>% 
  add_column(day=(1:tday)) %>% 
  add_column(day2=(1:tday)^2)%>% 
  add_column(GHS_Score=rep(GHS_Score,tday))%>% 
  add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
  add_column(UrbanPop=rep(UrbanPop,tday))
pred <- pred %>% 
  mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
newdat = readRDS("dat.rds")
newdat = newdat %>% filter(Country.Region==country_name)

ggplot()+
  geom_line(data=pred[1:40,],aes(x=day,y=model_mwg), col= "red") +
  geom_line(data=pred[40:tday,],aes(x=day,y=model_mwg), col= "red", linetype="dashed") +
  geom_line(data=newdat[1:40,], aes(x=day,y=new_cases))+
  geom_line(data=newdat[40:48,], aes(x=day,y=new_cases), linetype="dashed") +
  labs(x = "Day", y = "New Cases") + 
  ggtitle("USA Predictions with GLMM")

```

To assess how well our model performed in prediction compared to the glmer package, we can calculate the MSE over all countries for our implementation of the model and the glmer package. We get
```{r echo=FALSE , warning=FALSE, message=FALSE}
## more data processing
dat = readRDS("dat2.rds")
# remove na data
dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
# modify china new_cases day 0 since it was NA previously
dat[402,5]=548
dat$ID <- dat %>% group_indices(Country.Region)

for (i in 1:max(dat$ID)) {
  if (sum(dat$ID==i) < 5) {
    dat<- dat[!(dat$ID==i),]
  }
}

dat$ID <- dat %>% group_indices(Country.Region)

# unique country list
order = unique(dat$Country.Region)

glmm1 <- glmer(new_cases ~ day + day2 + GHS_Score + AgeGEQ65 + UrbanPop + (day | Country.Region), data = dat, family = poisson)
fix_glmer <- fixef(glmm1)

gamma <- read.table("longleaf/glmm_mwg_rw_gamma_1.txt", header = F, skip = 1)
gamma2 <- as.matrix(gamma[,2:3])
M <- 1000
fix_mwg <- c(0.844,0.202,-0.005,0.028,0.010,-0.001)

MSE <- 0

for(i in 1:99){
  country = i
  country_name = order[i]
  
  ran_glmer <- c((ranef(glmm1)$Country.Region[country,])[[1]],(ranef(glmm1)$Country.Region[country,])[[2]],0,0,0,0)
  ran_mwg <- c(mean(gamma2[((country-1)*M+1):(country*M),1]),mean(gamma2[((country-1)*M+1):(country*M),2]),0,0,0,0)
  coef_glmer <- fix_glmer + ran_glmer
  coef_mwg <- fix_mwg + ran_mwg
  
  dat2 <- dat %>% filter(Country.Region==country_name)
  
  tday = dim(dat2)[1]+8
  tdayp = tday-7
  GHS_Score <- ((dat %>% filter(Country.Region==country_name))[1,])$GHS_Score
  AgeGEQ65 <- ((dat %>% filter(Country.Region==country_name))[1,])$AgeGEQ65
  UrbanPop <- ((dat %>% filter(Country.Region==country_name))[1,])$UrbanPop
  pred <- tibble(int=rep(1,tday)) %>% 
    add_column(day=(1:tday)) %>% 
    add_column(day2=(1:tday)^2)%>% 
    add_column(GHS_Score=rep(GHS_Score,tday))%>% 
    add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
    add_column(UrbanPop=rep(UrbanPop,tday))
  pred <- pred %>%
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
  newdat = readRDS("dat.rds")
  newdat = newdat %>% filter(Country.Region==country_name)
  
  MSE <- MSE + sum((pred[tdayp:tday,]$model_glmer-newdat[tdayp:tday,]$new_cases)^2)
}
MSE <- MSE/(99*8)

MSE2 <- 0

for(i in 1:99){
  country = i
  country_name = order[i]
  
  ran_glmer <- c((ranef(glmm1)$Country.Region[country,])[[1]],(ranef(glmm1)$Country.Region[country,])[[2]],0,0,0,0)
  ran_mwg <- c(mean(gamma2[((country-1)*M+1):(country*M),1]),mean(gamma2[((country-1)*M+1):(country*M),2]),0,0,0,0)
  coef_glmer <- fix_glmer + ran_glmer
  coef_mwg <- fix_mwg + ran_mwg
  
  dat2 <- dat %>% filter(Country.Region==country_name)
  
  tday = dim(dat2)[1]+8
  tdayp = tday-7
  GHS_Score <- ((dat %>% filter(Country.Region==country_name))[1,])$GHS_Score
  AgeGEQ65 <- ((dat %>% filter(Country.Region==country_name))[1,])$AgeGEQ65
  UrbanPop <- ((dat %>% filter(Country.Region==country_name))[1,])$UrbanPop
  pred <- tibble(int=rep(1,tday)) %>% 
    add_column(day=(1:tday)) %>% 
    add_column(day2=(1:tday)^2)%>% 
    add_column(GHS_Score=rep(GHS_Score,tday))%>% 
    add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
    add_column(UrbanPop=rep(UrbanPop,tday))
  pred <- pred %>%
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
  newdat = readRDS("dat.rds")
  newdat = newdat %>% filter(Country.Region==country_name)
  
  MSE2 <- MSE2 + sum((pred[tdayp:tday,]$model_mwg-newdat[tdayp:tday,]$new_cases)^2)
}
MSE2 <- MSE2/(99*8)
```

```{r}
## glmer MSE
MSE
## Our model MSE
MSE2
```


## *Machine Learning*

### *Prophet Time-Series Modeling*

To build individual models for each country, we wrote a function to determine both model and forecast. 

```{r}
forecast <- function(country, data, numPred){
  dat <- data[data$Country.Region == country, ]
  prophet_dat <- as.data.frame(dat$date)
  prophet_dat$y <- as.numeric(dat$total_cases)
  prophet_dat$cap <- as.numeric(dat$TotalPop*.01)
  
  colnames(prophet_dat) <- c("ds", "y", "cap")
  
  now <- prophet(prophet_dat, growth= "logistic", daily.seasonality = F, yearly.seasonality = F)
  future <- make_future_dataframe(now, periods=numPred)
  future$cap <- rep(prophet_dat[1,3], length(future$ds))
  
  forecast <- predict(now, future, )
  print(plot(now, forecast, plot_cap=F, uncertainty = T, ylabel = paste(country, "total cases")))
  return(list(now,forecast))
}
```
This function has three inputs:

Country: a character value written exactly as the country is listed in the dataset
Data: Any dataframe that includes country name, total cases, and the country's total population. 
numPred: The number of days you want to predict in the future

The function returns a graph of the model and forecast as well as a dataframe that includes all previous data and forecasted predictions for the length of numPred. 

We now look at the results for a few countries. 
```{r, warning= FALSE}
dat <- readRDS("dat3.rds")
par(mfrow = c(2,2))
US <- forecast("US", dat, 10)
SouthKorea <- forecast("Korea, South", dat, 10)
Italy <- forecast("Italy", dat, 10)
China <- forecast("China", dat, 10)
```

We see in general, the model fits fairly well if the country has not reached its inflection point. You can see this from the graphs for the US and Italy. However, if the country has been effective at flattening the curve, such as China and South Korea, the method can not properly capture the inflection point nor make accurate forecasts for future cases.  


## *Comparison on Methods*

# Discussion
(give brief summary of what was shown in the results section and add in any subjective thoughts.Interpretation or possible future directions)


## *Limitations*
(This could go on for pages - will try to focus on a brief description. Discussed including one or two plots, such as South Korea, to show some limitations)

"It is useful at this point to reflect that modelling infectious diseases, particularly emerging pandemics, is different to modelling environmental or physical phenomena (such as climate change or fluid dynamics). This is because the underpinning knowledge or empirical data are often rare or highly uncertain and the timescales for intervention relatively fast." - Walters et. al. (2018)

Often when creating a model we look at models previously used for similar phenomena. COVID-19 could be considered to spread in a similar way to influenza. However, While there have been influenza outbreaks in recent history (1918, 1957, 1968, 2009), socio-cultural changes between each previously modeled pandemic, including movement patterns, advances in medicine, new forms of data collection, etc. make it very difficult, if not useless to use a model that was created for a previous pandemic. Since pandemics are fairly rare, there are not a lot of previously identified models that are sure to work. (Walters et al 2018)


De Angelis et. al. (2015) describe how Markov chain Monte Carlo (MCMC) approaches have become somewhat of a 'gold standard' in the world of modeling infectious diseases. However, they note several complications that we can fully understand after working through this project. These include: writing a likelihood in closed form, data augmentation that invlolves imputation of more unkowns than feasible to handle, and difficulty implementing the model in real time due to high level of computational effort.


We were not able to gather data on movement restrictions and when they were put into place for each country, as this data is not readily available. Including travel and contact patterns in a pandemic model is extremely important. However, this data is not readily available to the public. There are datasets that describe travel patterns, but according to Walters et. al. (2018), access to these data are often restricted, and even those who have access may see a delay in updates. This often leads to models being uninformative for a period of time.


- countries not accurately reporting data or lag time between cases confirmed and reported

- lack of widespread testing and possibility of infected individuals lacking symptoms


# References

“Coronavirus.” World Health Organization, World Health Organization, www.who.int/health-topics/coronavirus#tab=tab_1.

De Angelis, Daniela et al. “Four key challenges in infectious disease modelling using data from multiple sources.” Epidemics vol. 10 (2015): 83-7. doi:10.1016/j.epidem.2014.09.004

Google Cloud Platform, Google, console.cloud.google.com/marketplace/details/johnshopkins/covid19_jhu_global_cases.

Junling Ma, "Estimating epidemic exponential growth rate and basic reproduction number." Infectious Disease Modelling, Volume 5 (2020): 129-141. doi: 10.1016/j.idm.2019.12.009. eCollection 2020.

Ngufor, C., Van Houten, H., Caffo, B. S., Shah, N. D., & McCoy, R. G. (2019). Mixed Effect Machine Learning: a framework for predicting longitudinal change in hemoglobin A1c. Journal of biomedical informatics, 89, 56-67.

Taylor, S. J., & Letham, B. (2018). Forecasting at scale. The American Statistician, 72(1), 37-45.

“The Global Health Security Index.” GHS Index, www.ghsindex.org/. 

Walters, Caroline E et al. “Modelling the global spread of diseases: A review of current practice and capability.” Epidemics vol. 25 (2018): 1-8. doi:10.1016/j.epidem.2018.05.007

“World Bank Open Data.” Data, 7 Apr. 2020, data.worldbank.org/.


