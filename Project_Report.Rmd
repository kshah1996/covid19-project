---
title: ""
author: "Emily Damone, Taylor Krajewski, Alex Quinter, Kushal Shah, Euphy Wu, Jonathan Zhang"
date: "4/22/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(prophet)
library(lme4)
library(gridExtra)
```

<p align="center">
<img width="500" src="mask.png">
</p>



<font size = "4"><div align="center">**Emily Damone, Taylor Krajewski, Alex Quinter, Kushal Shah, Euphy Wu, Jonathan Zhang**</div></font>



# Introduction

COVID-19 (Novel Coronavirus) is an infectious disease that was first identified in December 2019 in Wuhan, the capital city of Hubei province in the People's Republic of China. Since its discovery, it has currently spread to over 90% of countries in the world and has infected nearly 2,000,000 people.

Despite movement restrictions put in place by most countries, the disease continues to spread, and new cases are documented daily. However, the lack of symptoms for many infected with coronavirus along with lack of widespread testing has made it nearly impossible to determine how many new cases of Coronavirus there actually are each day. This makes modeling the pandemic extremely difficult. 

Many experts have made excellent models and predictions that inspired us to create our own. We have seen some excellent country-specific, or even US state-specific models that attempt to predict the number of new or total cases. In the model we will describe below, we attempted to predict the number of new cases for a country based on that country's previous number of documented cases, global health security index, percentage of population over 65 years old, and percentage of the population living in an urban setting. 


# Project Goal

To predict the number of new cases of Covid-19 a country can expect on a chosen day given that country's previous number of recorded cases and a few measurable variables specific to that country that may affect the spread of the virus.


# Data

## *Raw Data*

### Hopkins COVID-19 Data

Our initial dataset comes from the Johns Hopkins CSSE, a team which has been refreshing the dataset daily during the ongoing COVID-19 crisis. The dataset includes coronavirus total cases, recoveries, active cases, and deaths per day by country, with information for certain regions given in more detail (e.g. US states). 

For this project, the focus was confirmed cases. Dates from the original dataset wre standardized by relabeling the date at which a country passes 50 total cases as "Day 0", with every day past that used for modeling purposes. A variable for new cases was created by lagging over total cases. 

### World Bank Data
According to the CDC, "older adults and people who have severe underlying medical conditions like heart or lung disease or diabetes seem to be at higher risk for developing more serious complications from COVID-19 illness." 
As of mid-March, in the United States, more than 30% of COVID-19 cases were patients 65 years of age or older. 

Due to the apparent prevalence of the disease in the 65+ population, we decided to include the percentage of a country's population over the age of 65 in our model. 

We found these percentages from a World Bank dataset, last updated in 2019, that provides the percent of the population 65 years of age or older in each country.

### Global Health Security Index Data

The Global Health Security (GHS) Index is a comprehensive assessment of a country's health security capabilities. Countries receive an ovreall score as well as individual scores for each of the 6 categories:

<p align="center">
<img width="600" src="ghs image.png">
</p>


* *Prevention*: Prevention of the emergence or release of pathogens

* *Detection and Reporting*: Early detection and reporting for epidemics of potential international concern

* *Rapid Response*: Rapid response to and mitigation of the spread of an epidemic

* *Health System*: Sufficient and robust health system to treat the sick and protect health workers

* *Compliance with International Norms*: Commitments to improving national capacity, financing plans to address gaps, and adhering to global norms

* *Risk Environment*: Overall risk environment and country vulnerability to biological threats

(*GHSIndex.org*)

As the GHS Index was created in response to the 2014 West Africa Ebola epidemic in order to help global leaders better understand and measure a country's ability to prevent, detect, and respond to infectious disease threats, we found it an appropriate measure to include in modeling the current Coronavirus pandemic.

For this project, we have included a country's overall GHS Index as a covariate in our model. 


## *Data Processing*

The Hopkins and GHS Index data were combined so that along with each country's case counts, we could use GHS Score, urban population percentage, and percentage of population older than 65. For project, the aforementioend variables were used along with country, day and number of new cases. 

## *Finalized Data*

Below is a preview of what our final data set looks like. Each country has a row for each day of recorded cases from baseline (day of 50 cases) until April 3, 2020. The first and last five days of our data set are displayed for the United States.

```{r include=FALSE}
library(data.table)
library(knitr)
library(FSA)

dat2 <- readRDS("dat2.rds")
datDisplay = readRDS("dat3.rds")

us <- subset(datDisplay, Country.Region == "US", select = c(day, total_cases, new_cases, GHS_Score, AgeGEQ65, UrbanPop, TotalPop))

  
colnames(us) <- c("Day", "Total Cases", "New Cases", "GHS Index", "% 65+", "% Urban Setting", "Total Population (2018)")


US <- headtail(us,5)
```

```{r, echo=FALSE}

kable(US, rnames = FALSE,
          caption="United States Data")

```


# Packaging
All functions needed to model and predict number of new cases of COVID-19 can be found in our R package, covid. Below is a description of each of these functions along with their code.

## Metropolis Within Gibbs Random Walk

## Graph a Country

This function creates a graph for user specified country that displays (in black) observed number of new cases of COVID-19 from their baseline day of 50 cases confirmed until April 3, 2020. The Poisson Generalized Linear Mixed Model is graphed over the same period (in red) to show accuracy of model in explaining number of new cases for that country. By specifiying glmer_results=TRUE, the glmer results are also graphed over the same period (in blue). An option to generate the predictions for the next 8 days is present as well. If set to true, predictions will be displayed as a dotted line extension of the GLMM (red) and glmer (blue) curves.

Function Inputs:

* `Country_Name`: A character input corresponding to the country being graphed

* `prediction`: A boolean input to generate the predictions over the next 8 days

* `Pred_Day`: A specified number of days past April 3rd the user wants to predict. Default is 8 days; only used when prediction is set to TRUE

* `glmer_results`: A boolean input to display results from glmer in blue


```{r, warning=FALSE, message=FALSE}
countrygraph <- function(Country_Name, prediction = FALSE, Pred_Day=NULL, glmer_results = FALSE){
  
  #ERROR CHECK#
  
  #Check Country_Name is a character
  if(class(Country_Name)!="character")
    stop("'Country_Name' must be a character input")
  
  #Check Pred_Day is an integer variable
  if(class(Pred_Day)!="numeric" || (class(Pred_Day)=="numeric" && Pred_Day != floor(Pred_Day)))
    stop("'Pred_Day' must be an integer input")
  
  #________________________________________#
  
  
  #FUNCTION#
  
  # Read in data
  dat = readRDS("dat2.rds")
  
  # Remove NA data
  dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
  
  # Modify china new_cases day 0 since it was NA previously
  dat[402,5]=548
  dat$ID <- dat %>% group_indices(Country.Region)
  
  # Remove countries with less than 5 cases
  for (i in 1:max(dat$ID)) {
    if (sum(dat$ID==i) < 5) {
      dat<- dat[!(dat$ID==i),]
    }
  }
  dat$ID <- dat %>% group_indices(Country.Region)
  
  # Create unique country list
  order = unique(dat$Country.Region)
  
  # Read in GLMM model results
  gamma <- read.table("longleaf/glmm_mwg_rw_gamma_1.txt", header = F, skip = 1)
  gamma2 <- as.matrix(gamma[,2:3])
  glmm1 <- suppressWarnings(glmer(new_cases ~ day + day2 + GHS_Score + AgeGEQ65 + UrbanPop + (day | Country.Region), data = dat, family = poisson))
  fix_glmer <- fixef(glmm1)
  
  # Find Country ID based on user specified Country Name
  country_info <- dat %>% 
    filter(Country.Region==Country_Name)
  
  country_ID = country_info$ID[1]
  
  # Set fixed effects for GLMM
  fix_mwg <- c(0.844,0.202,-0.005,0.028,0.010,-0.001)
  
  # Get random effects for GLMM
  M <- 1000
  ran_mwg <- c(mean(gamma2[((country_ID-1)*M+1):(country_ID*M),1]),mean(gamma2[((country_ID-1)*M+1):(country_ID*M),2]),0,0,0,0)
  ran_glmer <- c((ranef(glmm1)$Country.Region[country_ID,])[[1]],(ranef(glmm1)$Country.Region[country_ID,])[[2]],0,0,0,0)
  
  
  #Combine fixed and random effects for GLMM
  coef_mwg <- fix_mwg + ran_mwg
  coef_glmer <- fix_glmer + ran_glmer
  #Create graph for specified country
  dat2 <- dat %>% 
    filter(Country.Region==Country_Name) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop)) %>% 
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop))
  # New data with 8 new days
  newdat = readRDS("dat.rds")
  newdat = newdat %>% filter(Country.Region==Country_Name)
  
  tday = dim(dat2)[1]+8
  tdayp = tday-7
  GHS_Score <- ((dat %>% filter(Country.Region==Country_Name))[1,])$GHS_Score
  AgeGEQ65 <- ((dat %>% filter(Country.Region==Country_Name))[1,])$AgeGEQ65
  UrbanPop <- ((dat %>% filter(Country.Region==Country_Name))[1,])$UrbanPop
  pred <- tibble(int=rep(1,tday)) %>% 
    add_column(day=(1:tday)) %>% 
    add_column(day2=(1:tday)^2)%>% 
    add_column(GHS_Score=rep(GHS_Score,tday))%>% 
    add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
    add_column(UrbanPop=rep(UrbanPop,tday))
  pred <- pred %>%
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
  
  graph_newcases <- NULL
  if(!prediction){
    if(!glmer_results){
      graph_newcases <- ggplot(data=dat2)+
        #True number of new cases
        geom_line(aes(x=day,y=new_cases))+
        #GLMM number of new cases
        geom_line(aes(x=day,y=model_mwg), col= "red")+
        
        labs(title=Country_Name, y="New Cases", x="Days since baseline 50 cases")
    }else{
      graph_newcases <- ggplot(data=dat2)+
        #True number of new cases
        geom_line(aes(x=day,y=new_cases))+
        #GLMM number of new cases
        geom_line(aes(x=day,y=model_mwg), col= "red")+
        #GLMER number of new cases
        geom_line(aes(x=day,y=model_glmer), col= "blue")
        labs(title=Country_Name, y="New Cases", x="Days since baseline 50 cases")
    }

  }else if (!is_empty(Pred_Day)) {
      tday = dim(dat2)[1]+Pred_Day
      tdayp = tday-(Pred_Day-1)
      
      pred <- tibble(int=rep(1,tday)) %>% 
        add_column(day=(1:tday)) %>% 
        add_column(day2=(1:tday)^2)%>% 
        add_column(GHS_Score=rep(GHS_Score,tday))%>% 
        add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
        add_column(UrbanPop=rep(UrbanPop,tday))
      pred <- pred %>%
        mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
        mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
      if(!glmer_results){
        graph_newcases <- ggplot()+
          geom_line(data=pred[1:tdayp,],aes(x=day,y=model_mwg), col= "red") +
          # Predictions
          geom_line(data=pred[tdayp:tday,],aes(x=day,y=model_mwg), col= "red", linetype="dashed") +
          geom_line(data=newdat[1:tdayp,], aes(x=day,y=new_cases))+
          # Predictions
          geom_line(data=newdat[tdayp:min(tday,dim(dat2)[1]+8),], aes(x=day,y=new_cases), linetype="dashed")+
          labs(title=Country_Name, y="New Cases", x="Days since baseline 50 cases")
      }else{
        graph_newcases <- ggplot()+
          # glmm
          geom_line(data=pred[1:tdayp,],aes(x=day,y=model_mwg), col= "red") +
          # Predictions
          geom_line(data=pred[tdayp:tday,],aes(x=day,y=model_mwg), col= "red", linetype="dashed") +
          # glmer
          geom_line(data=pred[1:tdayp,],aes(x=day,y=model_glmer), col= "blue") +
          # Predictions
          geom_line(data=pred[tdayp:tday,],aes(x=day,y=model_glmer), col= "blue", linetype="dashed") +
          
          geom_line(data=newdat[1:tdayp,], aes(x=day,y=new_cases))+
          # Predictions
          geom_line(data=newdat[tdayp:min(tday,dim(dat2)[1]+8),], aes(x=day,y=new_cases), linetype="dashed")+
          labs(title=Country_Name, y="New Cases", x="Days since baseline 50 cases")
      }

  }else{
    if(!glmer_results){
      graph_newcases <- ggplot()+
        geom_line(data=pred[1:tdayp,],aes(x=day,y=model_mwg), col= "red") +
        # Predictions
        geom_line(data=pred[tdayp:tday,],aes(x=day,y=model_mwg), col= "red", linetype="dashed") +
        geom_line(data=newdat[1:tdayp,], aes(x=day,y=new_cases))+
        # Predictions
        geom_line(data=newdat[tdayp:tday,], aes(x=day,y=new_cases), linetype="dashed")+
        labs(title=Country_Name, y="New Cases", x="Days since baseline 50 cases")
    }else{
      graph_newcases <- ggplot()+
        # glmm
        geom_line(data=pred[1:tdayp,],aes(x=day,y=model_mwg), col= "red") +
        # Predictions
        geom_line(data=pred[tdayp:tday,],aes(x=day,y=model_mwg), col= "red", linetype="dashed") +
        # glmer
        geom_line(data=pred[1:tdayp,],aes(x=day,y=model_glmer), col= "blue") +
        # Predictions
        geom_line(data=pred[tdayp:tday,],aes(x=day,y=model_glmer), col= "blue", linetype="dashed") +
        
        geom_line(data=newdat[1:tdayp,], aes(x=day,y=new_cases))+
        # Predictions
        geom_line(data=newdat[tdayp:tday,], aes(x=day,y=new_cases), linetype="dashed")+
        labs(title=Country_Name, y="New Cases", x="Days since baseline 50 cases")
    }
  }
  return(graph_newcases)
}
```

## Random Forest

## Prophet
```{r}
forecast <- function(country, data, numPred){
  dat <- data[data$Country.Region == country, ]
  prophet_dat <- as.data.frame(dat$date)
  prophet_dat$y <- as.numeric(dat$total_cases)
  prophet_dat$cap <- as.numeric(dat$TotalPop*.01)
  
  colnames(prophet_dat) <- c("ds", "y", "cap")
  
  now <- prophet(prophet_dat, growth= "logistic", yearly.seasonality = F, daily.seasonality = F)
  future <- make_future_dataframe(now, periods=numPred)
  future$cap <- rep(prophet_dat[1,3], length(future$ds))
  
  forecast <- predict(now, future, )
  plot <- plot(now, forecast, plot_cap=F, uncertainty = T, ylabel = country, xlabel = "date") +  add_changepoints_to_plot(now)
  return(list(plot, forecast))
}
```
This function has three inputs:
  
`country`: a character value written exactly as the country is listed in the dataset  
`data`: Any dataframe that includes country name, total cases, and the country's total population    
`numPred`: The number of days you want to predict in the future  

The function returns a list including a graph of the model and forecast as well as a dataframe that includes all previous data and forecasted predictions for the length of numPred. 

# Methods

## *Motivation*

During the initial growth phase of a pandemic, the number of cases shows exponential growth. As time progresses, the number of new cases each day starts to decrease, and eventually we reach a final number of cases. Thus, the cumulative number of cases form an approximately linear relationship with time in the log-linear scale in the initial phases, but then plateaus after a certain timepoint.
It is common in Epidemiology to model this initial exponential growth and subsequent slow-down using a logistic growth model. The cumulative incidences $C(t)$ (the total number of cases by time $t$) can be approximated by

$$
C(t)=\frac{KC_0}{C_0+(K-C_0)e^{-rt}}
$$

where $r$ is the exponential growth rate, $K=\displaystyle \lim_{t \to \infty} C(t)$ and $C_0=C(0)$.

By taking the derivative of $C(t)$, we can estimate the change in number of total cases with respect to time. Hence, when looking between two subsequent time points, we can find the number of new cases.

The derivative of $C(t)$, the density of the logistic function, is $\frac{d}{dt}=\frac{rKC_0e^{rt}(K-C_0)}{((K-C_0)+C_0e^{rt})^2}$, which looks a lot like a normal distribution! Hence, we can use a normal distribution to approximate the logistic function. 


```{r, echo=FALSE}
# Graphing Logistic Growth, Derivative, and Gaussian Curve

#Logistic Growth (CDF)
L = function(x){
  plogis(x)
}

#Logistic Density (PDF)
DL = function(x){
    2*dlogis(x)
}

#Normal Density
N=function(x){
  2*dnorm(x,0,1.6)
}


x <- seq(-10, 10, by = 0.1)

colors <- c("purple", "orange", "blue")

#labels <- c("Total Cases (Logistic Growth Curve)","Derivative of Total Cases (Logistic Curve)", "Gaussian Curve")
labels <- c("Total Cases","New Cases", "Gaussian Curve")

#Plot Logistic Growth Curve
plot( L(x), yaxt='n', type="l", lty=1, xlab="Time (in days)",
  ylab=" ", main=paste("Graphical Representation of Relationship between" ,
     "\n Logistic Growth (Total Cases), Logistic Density (New Cases)", "\nand Normal Denisty"), col="purple", lwd=3)
# Plot Logistic Density
lines( DL(x), yaxt='n', col="orange", lwd=3)
# Plot Gaussian Density
lines( N(x), yaxt='n', col="blue", lwd=3, lty=2)
#Add legend
legend("topleft", inset=0, title="Distributions",
  labels, lwd=2, lty=c(1, 1, 2), col=colors)
#Add title
# title("Graphical Representation of Relationship between" ,
#      "\n Logistic Growth, Logistic Density, and Normal Denisty")

```

This led us to attempt to model number of new cases using a Gaussian curve.
We found this fit the number of new cases quite well for many countries, with some limitations of course. Since this density fit the number of new cases quite well in initial modeling, we decided to create a model that incorporates an $exp(time^2)$ term, and hence we included covariates for $time$ as well as $time^2$.


## *Poisson GLMM*

### Model Creation
Let $y_{ij}$ be the number of new cases country $i$ has on day $j$. 

We assume that each $y_{ij} \sim Poisson(\lambda_{ij})$, where the mean and variance of $y_{ij}$ is assumed to be some $\lambda_{ij}$

Let the covariate vector pertaining to the $i^{th}$ countrh in the $j^{th}$ month be defined as $x_{ij} = (1, t_{ij}, t^2_{ij}, GHS, AgeGEQ65, UrbanPop)$.

We assume an intercept, $\beta_0$, in our model, and thus we have $\beta = (\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6)^T$.

We assumed counts can be modeled with a Poisson GLMM such that $log(\lambda_{ij})=x_{ij}\beta+\gamma_{i1}+\gamma_{i2}t_{ij} = x_{ij}\beta+z_{ij}\gamma_i$, where $b_i$ is a vector of unobserved country-level random effects. We assume that

$$\left(\begin{array}{cc}\gamma_{i1} \\\gamma_{i2}\end{array}\right) \sim N_2\bigg(\left(\begin{array}{cc}0 \\0\end{array}\right),\left(\begin{array}{cc}g_{11} & g_{12} \\g_{21} & g_{22}\end{array}\right)\bigg)$$

So, in our model, $\gamma_{i1}$ is the deviation of the $i^{th}$ country from the baseline number of new cases, which we have set to 50, and $\gamma_{i2}$ is the deviation of the $i^{th}$ country from the average effect of the covariates in our model on number of new cases.


As in all GLMMs, the random effects are unobservable. Therefore, in order to obtain the likelihood, we must integrate them out.

Thus, we must perform separate integrals for each $\left(\begin{array}{cc}\gamma_{i1} \\\gamma_{i2}\end{array}\right)$, which share the common distribution dtermined by $G=\left(\begin{array}{cc}g_{11} & g_{12} \\g_{21} & g_{22}\end{array}\right)$.

Thus, we write the likelihood:

$$L(\beta, G | y, \gamma)=\prod^{n}_{i=1}\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i | 0, G) = \prod^{n}_{i=1}\int\bigg[\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i|0,G)\bigg]d\gamma_i$$

and the log-likelihood:

$$l(\beta, G | y, \gamma)=\sum^{n}_{i=1}log\bigg[\int\bigg[\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i|0,G)\bigg]d\gamma_i\bigg]$$

where $f(y_{ij}|\lambda_{ij})$ is the Poisson PMF with mean $\lambda_{ij}$, as defined above, and $\phi(\gamma_i|0,G)$ is the bivariate Normal PDF with mean 0 and covariance matrix $G=\left(\begin{array}{cc}g_{11} & g_{12} \\g_{21} & g_{22}\end{array}\right)$.


We maximized this log-likelihood to obtain estimates for $\beta$ and $G$. We go about this approach using an MCEM algorithmn. We first define the complete data log-likelihood if the random effects were known.

\begin{align}
l_c(\beta, G | y, \gamma)&=\sum^{n}_{i=1}log\bigg[\bigg(\prod^{n_i}_{j=1}f(y_{ij}|\lambda_{ij})\bigg)\phi(\gamma_i|0,G)\bigg]\\

&=\sum^{n}_{i=1}\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ij}))+log(\phi(\gamma_i|0,G))\bigg]
\end{align}

In the E-step we need to evaluate the expectation of the complete data log-likelihood, the Q function.

\begin{align}
Q(\theta|\theta^{(t)})&=E[l_c(\theta)|\theta^{(t)}]\\

&=E\bigg[\sum^{n}_{i=1}\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ij}^{(t)}))+log(\phi(\gamma_i|0,G^{(t)}))\bigg]\bigg]\\

&=\sum^{n}_{i=1}\bigg[\int\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ij}^{(t)}))+log(\phi(\gamma_i|0,G^{(t)}))\bigg]f(\gamma_i|\beta^{(t)},G^{(t)})d\gamma_i\bigg]
\end{align}

where $\lambda_{ij}^{(t)}=e^{x_{ij}\beta^{(t)}+z_{ij}\gamma_{i}}$. If we could sample from the posterior distribution of $\gamma_i$ we could approximate this intergral through a MC approach.

\begin{align}
Q(\theta|\theta^{(t)})&=\frac{1}{M}\sum^{n}_{i=1}\bigg[\sum^{M}_{k=1}\bigg[\sum^{n_i}_{j=1}log(f(y_{ij}|\lambda_{ijk}^{(t)}))+log(\phi(\Gamma_{ik}|0,G^{(t)}))\bigg]\bigg]\\
\end{align}

where $\Gamma_{ik}$ is one of the $M$ samples from the $i$-th country, and $\lambda_{ijk}^{(t)}=e^{x_{ij}\beta^{(t)}+z_{ij}\Gamma_{ik}}$. To sample from this multivariate posterior, we tried 3 different approaches: an independence random walk, a Metropolis-within-Gibbs random walk, and an adaptive Metropolis-within-Gibbs random walk. Of these 3 methods, we decided that the Metropolis-within-Gibbs random walk approach was the most suitable. We consider a new $X^*=x^{(t)}+\epsilon$ at each Gibbs step, where $\epsilon\sim N(0,0.01)$. Then we using the Metropolis Hastings ratio to determine if it is accepted or rejected into the Markov Chain.

Now for the M-step, once we have the Q-function, we need to maximize it with respect to $\beta$ and $G$. We see from the Q-function above, that after we draw the $M$ samples for each $i$, $\Gamma_{ik}$, the maximization process of $\beta$ follows the maximization of parameters in a Poisson GLM with a few tweaks. First, each sample must be weighted $\frac{1}{M}$. Second, we need to add an offset that are the random effects $z_i\gamma_i$. Then using the glm function in R, we can perform the M-step for $\beta$. For the parameters in $G$, we use the sample variance/covariance of the Markov chains that we created. Afterwards, we start the E-step again and iterate until convergence.

### Model Prediction

To evaluate our model, we compare the predictions that our models made to the data from future days which were not used in the generation of our model. To facilitate this we need to be able to generate a prediction of the random effects given a country, or more specifically be able to estimate $\gamma_i|y_i,\beta,G$. We can use the following expectation as a good estimate.

\begin{align}
\hat{\gamma_i}&=E[\gamma_i|y_i,\hat{\beta},\hat{G}]\\
&=\int\gamma_if(\gamma_i|y_i,\hat{\beta},\hat{G})d\gamma_i
\end{align}
As in the E-step we can use a MC approach to estimate this mean as follows.
\begin{align}
E[\gamma_i|y_i,\hat{\beta},\hat{G}]&=\frac{1}{M}\sum^{M}_{k=1}\Gamma_{ik}
\end{align}

where $\Gamma_{ik}$ is one of the $M$ samples from the $i$-th country. Since we have already generated $\Gamma_{ik}$ from the E-step iterations above, we can take the mean of those Markov chains to estimate country specific random effects. The predicted number of new cases for country $i$ at day $j$ is

\begin{align}
E[y_{ij}|\gamma_i]&=\frac{1}{1+e^{-(x_{ij}\hat{\beta}+z_{ij}\hat{\gamma_i})}}
\end{align}

## *Machine Learning Methods*


Two modeling techniques were considered for the machine learning section: Random Forest and Prophet (time-series modeling). 

### Random Forest

There is some existing literature for machine learning methods for longitudinal data, however, the extent is limited and recent method developments lack direct applications in R. For example, Ngufor et al. (2018) develop a "Mixed Effect Random Forest", but its implementation in R is still limited and produces vague error messages. 

Therefore, although country-specific random effects could not be incorporated, a regular random forest technique was applied to this dataset instead, as it is still a nonparametric technique and may be able to uncover the temporal structure of the dataset. We used `day`, `GHS_Score`, `AgeGEQ65`, and `UrbanPop` to predict new cases, with the hope that the country-specific variables may result in more country-specific predictions.

### Prophet

Additionally, we decided to consider a time-series modeling method called Prophet to further answer our scientific question of predicting new cases in future days. Prophet was originally developed by Facebook Research and is available in packages for both R and Python. The method was built for building time-series predictions for business forecasting problems, but we believe it may be used to predict future cases of Covid-19. Note, this method can only be used to model cases for one country at a time.

Prophet is based in a decomposable time series model as follows: 
$$y(t) = g(t) + s(t) + h(t) +\epsilon_t$$
$y(t)$ ~ total cases, $g(t)$ ~ non-periodic changes, $s(t)$~periodic changes (such as weekly or yearly), $h(t)$~holiday effects, $\epsilon_t$~$N(0,\sigma^2)$

While the method allows for seasonal and holiday effects, we do not include such in our model as we do not believe it applies to Covid-19 cases. 

We utilized a logistic growth model, which has the following form: 
$$g(t) = \frac{C(t)}{1 + \exp(-k(t-m))}$$ Where $C(t)$ is a function for the carrying capacity, $k(t)$ is the growth rate, and $m$ is an offset parameter. In typical time series models, C and k are constants, but an advantage of Prophet is allowing these to vary with time.

Prophet utilizes *L-BFGS* to determine parameter estimates. 

# Results

## *Poisson GLMM*

Running the model we have on the dataset lead to parameter estimates as follows (code not explicity ran here, it was ran on the cluster).

```{r eval=FALSE}
mwgrw(beta = NA, Sigma_gamma = NA, M=1000, Z=c(1,2), control=list(tol=10^-5,maxit=1000), verbose= TRUE)
```

The NA for the starting values will tell the function to use the default values. The last 3 lines of the results are shown below.

```
Iter: 65 Qf: -86681.995 g11: 1.896946 g12: -0.067057 g22: 0.010074 beta0: 0.846 beta1:0.202 beta2:-0.005 beta3:0.028 beta4 :0.010 beta5:-0.001 eps:0.000104
Iter: 66 Qf: -86700.912 g11: 1.928345 g12: -0.068573 g22: 0.010125 beta0: 0.846 beta1:0.202 beta2:-0.005 beta3:0.028 beta4 :0.010 beta5:-0.001 eps:0.000218
Iter: 67 Qf: -86700.985 g11: 1.922342 g12: -0.067895 g22: 0.010088 beta0: 0.844 beta1:0.202 beta2:-0.005 beta3:0.028 beta4 :0.010 beta5:-0.001 eps:0.000001
```

with final parameter estimates of $\hat{\beta}=(0.844,0.202,-0.005,0.028,0.010,-0.001)$ and $\hat{G}=\left(\begin{array}{cc}1.922 & -0.068 \\-0.068 & 0.010\end{array}\right)$.

We can compare this to estimates from the glmer package in R.

```{r echo=FALSE}
## more data processing
dat = readRDS("dat2.rds")
# remove na data
dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
# modify china new_cases day 0 since it was NA previously
dat[402,5]=548
dat$ID <- dat %>% group_indices(Country.Region)

for (i in 1:max(dat$ID)) {
  if (sum(dat$ID==i) < 5) {
    dat<- dat[!(dat$ID==i),]
  }
}

dat$ID <- dat %>% group_indices(Country.Region)
```
```{r}
suppressWarnings(glmm1 <- glmer(new_cases ~ day + day2 + GHS_Score + AgeGEQ65 + UrbanPop + (day | Country.Region), data = dat, family = poisson))
summary(glmm1)
```

We see that the estimates are very close to what glmer reports. The difference could be attributed to the Laplace approximation that the glmer package uses or to the acceptance rates of our chains. In certain countries, the acceptance rate was very good and at 50%, but or other countries it was in the single digits. We tried adaptive methods to try to remedy this as well, but they did not perform any better than the standard Metropolis within Gibbs random walk. The chains could have been run for longer as well, $M>1000$, but due to the large computation costs it was already bordering on unfeasible given our time frame.

As can be seen in the glmer summary above, both $day$ and $day^{2}$ are highly significant in our model. Looking at the fixed covariates we added for Global Health Security Index (GHS_Score), percentage of population over the age of 65 (AgeGEQ65), and percentage of population living in an urban setting (UrbanPop), we see that, of these three, only a country's Global Health Security Index was significant in our model. Despite our assumption based on current literature that a large proportion of a population being over 65 years old or living in an urban setting could impact number of cases of COVID-19, we find that these predictors are not actually significant in our model.

Below in red are the predictions of our GLMM model for four countries, the United States, China, South Korea, and Italy. The model was built on data up until April, 03 2020, and then we use the predictions to project the next 8 days. Since the original data is from the past, we also know the true observed number of new cases in these countries as well (displayed in black). The dashed lines indicated time points that were not included in the generation of our model, and the blue line indicates the predictions provided by the glmer function.


```{r warning=FALSE}
US <- countrygraph("US", prediction = TRUE, Pred_Day = 8, glmer_results = TRUE)
China <- countrygraph("China", prediction = TRUE, Pred_Day = 8, glmer_results = TRUE)
SK <- countrygraph("Korea, South", prediction = TRUE, Pred_Day = 8, glmer_results = TRUE)
Italy <- countrygraph("Italy", prediction = TRUE, Pred_Day = 8, glmer_results = TRUE)
grid.arrange(US, China, SK, Italy, ncol = 2)
```

To assess how well our model performed in prediction compared to the glmer package, we can calculate the MSE over all countries for our implementation of the model and the glmer package. We get
```{r echo=FALSE , warning=FALSE, message=FALSE}
## more data processing
dat = readRDS("dat2.rds")
# remove na data
dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
# modify china new_cases day 0 since it was NA previously
dat[402,5]=548
dat$ID <- dat %>% group_indices(Country.Region)

for (i in 1:max(dat$ID)) {
  if (sum(dat$ID==i) < 5) {
    dat<- dat[!(dat$ID==i),]
  }
}

dat$ID <- dat %>% group_indices(Country.Region)

# unique country list
order = unique(dat$Country.Region)

glmm1 <- glmer(new_cases ~ day + day2 + GHS_Score + AgeGEQ65 + UrbanPop + (day | Country.Region), data = dat, family = poisson)
fix_glmer <- fixef(glmm1)

gamma <- read.table("longleaf/glmm_mwg_rw_gamma_1.txt", header = F, skip = 1)
gamma2 <- as.matrix(gamma[,2:3])
M <- 1000
fix_mwg <- c(0.844,0.202,-0.005,0.028,0.010,-0.001)

MSE <- 0

for(i in 1:99){
  country = i
  country_name = order[i]
  
  ran_glmer <- c((ranef(glmm1)$Country.Region[country,])[[1]],(ranef(glmm1)$Country.Region[country,])[[2]],0,0,0,0)
  ran_mwg <- c(mean(gamma2[((country-1)*M+1):(country*M),1]),mean(gamma2[((country-1)*M+1):(country*M),2]),0,0,0,0)
  coef_glmer <- fix_glmer + ran_glmer
  coef_mwg <- fix_mwg + ran_mwg
  
  dat2 <- dat %>% filter(Country.Region==country_name)
  
  tday = dim(dat2)[1]+8
  tdayp = tday-7
  GHS_Score <- ((dat %>% filter(Country.Region==country_name))[1,])$GHS_Score
  AgeGEQ65 <- ((dat %>% filter(Country.Region==country_name))[1,])$AgeGEQ65
  UrbanPop <- ((dat %>% filter(Country.Region==country_name))[1,])$UrbanPop
  pred <- tibble(int=rep(1,tday)) %>% 
    add_column(day=(1:tday)) %>% 
    add_column(day2=(1:tday)^2)%>% 
    add_column(GHS_Score=rep(GHS_Score,tday))%>% 
    add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
    add_column(UrbanPop=rep(UrbanPop,tday))
  pred <- pred %>%
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
  newdat = readRDS("dat.rds")
  newdat = newdat %>% filter(Country.Region==country_name)
  
  MSE <- MSE + sum((pred[tdayp:tday,]$model_glmer-newdat[tdayp:tday,]$new_cases)^2)
}
MSE <- MSE/(99*8)

MSE2 <- 0

for(i in 1:99){
  country = i
  country_name = order[i]
  
  ran_glmer <- c((ranef(glmm1)$Country.Region[country,])[[1]],(ranef(glmm1)$Country.Region[country,])[[2]],0,0,0,0)
  ran_mwg <- c(mean(gamma2[((country-1)*M+1):(country*M),1]),mean(gamma2[((country-1)*M+1):(country*M),2]),0,0,0,0)
  coef_glmer <- fix_glmer + ran_glmer
  coef_mwg <- fix_mwg + ran_mwg
  
  dat2 <- dat %>% filter(Country.Region==country_name)
  
  tday = dim(dat2)[1]+8
  tdayp = tday-7
  GHS_Score <- ((dat %>% filter(Country.Region==country_name))[1,])$GHS_Score
  AgeGEQ65 <- ((dat %>% filter(Country.Region==country_name))[1,])$AgeGEQ65
  UrbanPop <- ((dat %>% filter(Country.Region==country_name))[1,])$UrbanPop
  pred <- tibble(int=rep(1,tday)) %>% 
    add_column(day=(1:tday)) %>% 
    add_column(day2=(1:tday)^2)%>% 
    add_column(GHS_Score=rep(GHS_Score,tday))%>% 
    add_column(AgeGEQ65=rep(AgeGEQ65,tday))%>% 
    add_column(UrbanPop=rep(UrbanPop,tday))
  pred <- pred %>%
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
  newdat = readRDS("dat.rds")
  newdat = newdat %>% filter(Country.Region==country_name)
  
  MSE2 <- MSE2 + sum((pred[tdayp:tday,]$model_mwg-newdat[tdayp:tday,]$new_cases)^2)
}
MSE2 <- MSE2/(99*8)
```

```{r}
## glmer MSE
MSE
## Our model MSE
MSE2
```


## *Machine Learning*

### *Random Forest*

After running the random forest model, we observed that fits to the data and predictions were very inaccurate, especially for countries whose curves differed considerably from other countries.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(randomForest)

getRF <- function(Country_Name, Pred_Day = 8) {
  
  # Read in data
  dat2 <- readRDS("dat2.rds") %>%
    mutate(new_cases = replace(new_cases, Country.Region == "China" & day==0, 0))
  
  #ERROR CHECK#
  
  #Check Country_Name is a character
  if(class(Country_Name)!="character")
    stop("'Country_Name' must be a character input")
  
  #Check Country_Name valid
  if (!(Country_Name %in% dat2$Country.Region))
    stop("'Country_Name' must be a valid country")
  
  #Check Pred_Day is an integer variable
  if(!is_empty(Pred_Day) && (class(Pred_Day)!="numeric" || (class(Pred_Day)=="numeric" && Pred_Day != floor(Pred_Day))))
    stop("'Pred_Day' must be an integer input")
  
  # Run RF model on full dataset
  dat2_rf <- dat2 %>% select(Country.Region, day, GHS_Score, AgeGEQ65, UrbanPop, new_cases) %>%
  drop_na()
  dat2_rf_X <- dat2_rf %>% select(day, GHS_Score, AgeGEQ65, UrbanPop)
  dat2_rf_Y <- dat2_rf %>% select(new_cases)
  rf1 <- randomForest(x = dat2_rf_X, y = dat2_rf$new_cases)
  
  # Dataset with RF predictions
  dat2_rf_preds <- dat2_rf %>% mutate(pred = predict(rf1))
  dat2_rf_preds_country <- dat2_rf_preds %>% filter(Country.Region == Country_Name)
  
  # Manually create dataset for future predictions
  day <- 0:(max(dat2_rf_preds_country$day) + Pred_Day)
  GHS_Score <- dat2_rf_preds_country$GHS_Score[1]
  AgeGEQ65 <- dat2_rf_preds_country$AgeGEQ65[1]
  UrbanPop <- dat2_rf_preds_country$UrbanPop[1]
  
  # Finalize dataframe with future predictions
  prediction_df <- data.frame(day, GHS_Score, AgeGEQ65, UrbanPop)
  prediction_df$pred <- predict(rf1, prediction_df)
  prediction_df$new_cases <- c(dat2_rf_preds_country$new_cases, rep(NA, Pred_Day))
  
  # Plot country's cases and predictions
  ggplot(prediction_df, aes(x = day)) +
    geom_point(aes(y = new_cases)) +
    geom_line(aes(y = pred)) +
    labs(x = "Day", y = "New Cases") + 
    ggtitle(paste0(Country_Name, " Predictions (RF)"))
}
```

```{r, message=FALSE, warning=FALSE}
p1 <- getRF("US", 8)
p2 <- getRF("China", 8)
p3 <- getRF("Korea, South", 8)
p4 <- getRF("Italy", 8)

grid.arrange(p1, p2, p3, p4, ncol=2)
```


For example, for the United States, as plotted above, the random forest model highly overpredicts early on and underpredicts as the situation worsens. Whereas the GLMM fit for the US was quite accurate (even with the US being quite an outlier in terms of number of daily new cases) because it was prespecified to be a bell curve, random forest struggles because its estimates for the United States are biased towards the outcome of all countries with a similar `GHS_Score`, `AgeGEQ65`, and `UrbanPop`. 

### *Prophet Time-Series Modeling*

To build individual models for each country, we wrote a function to determine both model and forecast which we will utilize below. 


We now look at the results for a few countries. 
```{r}
dat <- readRDS("dat3.rds")
dat_limit <- dat[dat$date <= "2020-04-03",]
dat_current <- dat[dat$date > "2020-04-03",]

US <- forecast("US", dat_limit, 8)
SouthKorea <- forecast("Korea, South", dat_limit, 8)
Italy <- forecast("Italy", dat_limit, 8)
China <- forecast("China", dat_limit, 8)
```

```{r}
grid.arrange(US[[1]], SouthKorea[[1]], Italy[[1]], China[[1]], ncol=2)
```

We see in general, the model fits fairly well if the country has not reached its inflection point. You can see this from the graphs for the US and Italy. However, if the country has been effective at flattening the curve, such as China and South Korea, the method can not properly capture the inflection point nor make accurate forecasts for future cases.  

To use later on, the MSEs of prediction for each of the four models were calculated to compare with the two other methods used. 

```{R, warning=F, message=F, echo=F}
prophet_MSE <- function(country, forecast){
pred <- forecast[[2]]$yhat[forecast[[2]]$ds > "2020-04-03"]
true <- dat_current$total_cases[dat_current$Country.Region == country]

MSE <- mean((pred-true)^2)
return(MSE)
}


US_mse_prophet <- prophet_MSE("US", US)
SKorea_mse_prophet <- prophet_MSE("Korea, South", SouthKorea)
Italy_mse_prophet <- prophet_MSE("Italy", Italy)
China_mse_prophet <- prophet_MSE("China", China)
```

__I added this table here to better display all the MSEs. I figured we could move to later in the discussion if preferred. __


Country   | Poisson GLMM | GLMER  |  Random Forest  |  Prophet 
--------- | ------------ | ------ |--------------   |  ------------------------
US        |              |        |                 | `r US_mse_prophet`
S. Korea  |              |        |                 |  `r SKorea_mse_prophet`
Italy     |              |        |                 |`r Italy_mse_prophet`
China     |              |        |                 | `r China_mse_prophet`




# Discussion

## *Comparison of Methods*

In this setting, a random forest modeling approach (and the majority of available machine learning approaches) is limited by its inability to handle correlated, longitudinal data.

One key limitation of random forest for this dataset is the fact that countries with similar GHS scores and population structures still had vastly different disease curves and trajectories with respect to new COVID cases per day. It is nearly impossible to gather a collection of covariates that accounts for the differences in curve structure between every country. Whether such a set of covariates even exists is unlikely, let alone actually finding it. Therefore, without being able to predict for an individual country based off of its previous time points (as was the case with the longitudinal GLMM modeling approach), making country-specific predictions through a random forest model was unlikely to be fruitful.

Additionally, although random forest's nonparametric modeling structure may seem promising here, we noticed that generally, the number of new cases for the majority of countries follows a bell-shaped curve with time, although with vastly different parameters that specify the curve. This situation is indeed well-suited for parametric statistical methods that limit each country's model to a specified distribution, while still allowing the parameters of that distribution to vary wildly. Extra assumptions that were needed for GLMM, in this case, were helpful, because they ensured, for example, that the predicted new cases for any single day is nonnegative. This rigidity resulted in extrapolations (future time points) at least being reasonable.

On the other hand, our time-series modeling approach, Prophet, suffered from the opposite problem that random forest did. Unlike random forest struggling to incorporate a country's previous values into the modeling, Prophet could *only* use a country's previous time points to predict future time points, and was unable to incoroporate information from other countries' disease curves. Therefore, Prophet could only be used reliably on countries with a large amount of time points, making global predictions impossible.

In one sense, the GLMM approach takes the best from both worlds: it allows for flexibility where it is needed (in variation amongst the country-specific parameters), while also remaining rigid in a way that leads to reasonable predictions (giving every country the same bell-curve shape, which is a valid assumption based on disease literature and is reflected in the COVID data so far). To make future predictions for time points outside the current dataset, fixing a bellcurve-type structure helps.

## *Limitations*

### Situational Limitations

"It is useful at this point to reflect that modelling infectious diseases, particularly emerging pandemics, is different to modelling environmental or physical phenomena (such as climate change or fluid dynamics). This is because the underpinning knowledge or empirical data are often rare or highly uncertain and the timescales for intervention relatively fast." - Walters et. al. (2018)


Often when creating a model we look at models previously used for similar phenomena. COVID-19 could be considered to spread in a similar way to influenza. However, While there have been influenza outbreaks in recent history (1918, 1957, 1968, 2009), socio-cultural changes between each previously modeled pandemic, including movement patterns, advances in medicine, new forms of data collection, etc. make it very difficult, if not useless to use a model that was created for a previous pandemic. Since pandemics are fairly rare, there are not a lot of previously identified models that are sure to work. (Walters et al 2018)


Another limitation we faced was inability to gather data on movement restrictions and when they were put into place for each country, as this data is not readily available. Including travel and contact patterns in a pandemic model is extremely important. However, this data is not readily available to the public. There are datasets that describe travel patterns, but according to Walters et. al. (2018), access to these data are often restricted, and even those who have access may see a delay in updates. This often leads to models being uninformative for a period of time.


Additionally, lack of widespread testing and the possibility of infected individuals lacking symptoms make it impossible to know how many cases of COVID-19 there truly are, and hence we face limitations in evaluating the true accuracy of our model.


Finally, countries not accurately reporting data on cases of COVID-19, lag time between cases confirmed and cases reported, or major changes in diagnoses criteria can make model fitting quite difficult. For example, on February 6th, officials in China’s Hubei province revealed that new cases of COVID-19 had surged nearly nearly 10x more than the day before—reversing days of declines that experts had thought were a positive sign in terms of containment of the virus. The number of deaths on this day also increased drastically. The jump in new cases was nearly 15,000 and was due to a change in the criteria for counting diagnoses of the virus - China added a new category of ‘clinical cases’ to its reporting so that patients who exhibit all the symptoms of COVID-19 - including fever, cough, and shortness of breath - but have either not been tested or tested negative for the virus itself, would still be considered cases. As can be seen in the graph of China's new cases over time, this spike in number of new cases on February 6th due to a new classification of cases can make any model fit seem inaccurate. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## more data processing
dat = readRDS("dat2.rds")
# remove na data
dat <- dat %>% mutate(day2 = day^2) %>% drop_na(GHS_Score) %>% drop_na(AgeGEQ65) %>% drop_na(UrbanPop)
# modify china new_cases day 0 since it was NA previously
dat[402,5]=548
dat$ID <- dat %>% group_indices(Country.Region)

for (i in 1:max(dat$ID)) {
    if (sum(dat$ID==i) < 5) {
        dat<- dat[!(dat$ID==i),]
    }
}

dat$ID <- dat %>% group_indices(Country.Region)

# unique country list
order = unique(dat$Country.Region)

glmm1 <- glmer(new_cases ~ day + day2 + GHS_Score + AgeGEQ65 + UrbanPop + (day | Country.Region), data = dat, family = poisson)
fix_glmer <- fixef(glmm1)

gamma <- read.table("longleaf/glmm_mwg_rw_gamma_1.txt", header = F, skip = 1)
gamma2 <- as.matrix(gamma[,2:3])
M <- 1000
# mean(gamma2[(49*M+1):(50*M),1])
# mean(gamma2[(49*M+1):(50*M),2])

fix_mwg <- c(0.844,0.202,-0.005,0.028,0.010,-0.001)





##### China
country = 21
country_name = 'China'

ran_glmer <- c((ranef(glmm1)$Country.Region[country,])[[1]],(ranef(glmm1)$Country.Region[country,])[[2]],0,0,0,0)
ran_mwg <- c(mean(gamma2[((country-1)*M+1):(country*M),1]),mean(gamma2[((country-1)*M+1):(country*M),2]),0,0,0,0)
coef_glmer <- fix_glmer + ran_glmer
coef_mwg <- fix_mwg + ran_mwg

dat2 <- dat %>% 
    filter(Country.Region==country_name) %>% 
    mutate(model_glmer=exp(coef_glmer[1]+coef_glmer[2]*day+coef_glmer[3]*day^2+coef_glmer[4]*GHS_Score+coef_glmer[5]*AgeGEQ65+coef_glmer[6]*UrbanPop)) %>% 
    mutate(model_mwg=exp(coef_mwg[1]+coef_mwg[2]*day+coef_mwg[3]*day^2+coef_mwg[4]*GHS_Score+coef_mwg[5]*AgeGEQ65+coef_mwg[6]*UrbanPop))
ggplot(data=dat2)+
    geom_line(aes(x=day,y=new_cases))+ labs(title = "China", x="Days since baseline 50 cases", y="New Cases")

```




### Statistical Limitations

De Angelis et. al. (2015) describe how Markov chain Monte Carlo (MCMC) approaches have become a 'gold standard' in the world of modeling infectious diseases. However, they note several complications that we can fully understand after working through this project. These include: writing a likelihood in closed form, data augmentation that invlolves imputation of more unkowns than feasible to handle, and difficulty implementing the model in real time due to high level of computational effort.

These computational limitations can be seen when running our Poisson GLMM code. We ran the modeling on longleaf and it took 3 hours to complete. With real-time data it is not out of the ordinary to run models at least once a day if not more frequently depending on the reliability of the data coming in. When one is attempting to capture the true nature of the outbreak in real time it is critical to have models which incorperate accurate, up-to-date information and receive results with a turnaround time that is still applicable in the situation. 

We have also seen a limitation on our normality assumption. As more data comes in, countries who have reached their peak in case counts seem to be taking a more linear or right-skewed decrease in the number of new cases. 

```{R, message=F, warning=F, echo=F}
p1 <- dat[dat$Country.Region == "Korea, South", ] %>% ggplot(aes(x=day, y=new_cases)) + 
                                                geom_line(aes(color="True Cases")) + 
                                                geom_smooth( se = F, aes(color="Smoothed Cases")) +
                                                labs(title = "South Korea")+ xlab("Day") + ylab("New cases")+
                                                scale_color_manual(name="legend", values=c("blue", "black"))
  
p2 <- dat[dat$Country.Region == "Italy", ] %>% ggplot(aes(x=day, y=new_cases)) + 
                                                geom_line() + 
                                                geom_smooth( se = F) +
                                                labs(title = "Italy")+ xlab("Day") + ylab("New cases")
grid.arrange(p1,p2, nrow=2)
```

As you can see above with the data from South Korea and Italy, the plots for new cases are right-skewed instead of following the assumed bell curve shape. This may lead to underpredictions of new case count data for countries that have reached their peak. Note that the black line represents true count data and the blue curve is simply a smoothing curve, not a distributional curve. 


# References

“Coronavirus.” World Health Organization, World Health Organization, www.who.int/health-topics/coronavirus#tab=tab_1.

De Angelis, Daniela et al. “Four key challenges in infectious disease modelling using data from multiple sources.” Epidemics vol. 10 (2015): 83-7. doi:10.1016/j.epidem.2014.09.004

Google Cloud Platform, Google, console.cloud.google.com/marketplace/details/johnshopkins/covid19_jhu_global_cases.

Ma, J. "Estimating epidemic exponential growth rate and basic reproduction number." Infectious Disease Modelling, Volume 5 (2020): 129-141. doi: 10.1016/j.idm.2019.12.009. eCollection 2020.

Ngufor, C., Van Houten, H., Caffo, B. S., Shah, N. D., & McCoy, R. G. (2019). Mixed Effect Machine Learning: a framework for predicting longitudinal change in hemoglobin A1c. Journal of biomedical informatics, 89, 56-67.

Taylor, S. J., & Letham, B. (2018). Forecasting at scale. The American Statistician, 72(1), 37-45.

“The Global Health Security Index.” GHS Index, www.ghsindex.org/. 

Walters, Caroline E et al. “Modelling the global spread of diseases: A review of current practice and capability.” Epidemics vol. 25 (2018): 1-8. doi:10.1016/j.epidem.2018.05.007

“World Bank Open Data.” Data, 7 Apr. 2020, data.worldbank.org/.


